## 1ï¸âƒ£Pre-training tasks([**Referenced from**](https://link.springer.com/content/pdf/10.1007/s11431-020-1647-3.pdf))

1. ### ğŸ˜€Language modeling (LM)

   A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length *m*, it assigns a probability $P(w_1, ... , w_m) $ to the whole sequence.  

2. ### ğŸ˜‚Masked language modeling (MLM)

   MLM first masks out some tokens from the input sentences and then trains the model to predict the masked tokens by the rest of the tokens. (å¯ä»¥è®¤ä¸ºæ˜¯DAEçš„ä¸€ç§)

   - ğŸ„â€â™‚ï¸Enhanced masked language modeling (E-MLM): 

     there are multiple research proposing different enhanced versions of MLM to further improve on BERT.UniLM extends the task of mask prediction on three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. XLM performs MLM on a concatenation of parallel bilingual sentence pairs, called Translation Language Modeling (TLM). SpanBERT replaces MLM with Random Contiguous Words Masking and Span Boundary Objective (SBO) to integrate structure information into pre-training, which requires the system to predict masked spans based on span boundaries. Besides, StructBERT introduces the Span Order Recovery task to further incorporate language structures.Another way to enrich MLM is to incorporate external knowledge.

3. ### ğŸ˜ŠPermuted language modeling (PLM)

   PLM is a language modeling task on a random permutation of input sequences. A permutation is randomly sampled from all possible permutations. Then some ofthe tokens in the permuted sequence are chosen as the target, and the model is trained to predict these targets, depending on the rest of the tokens and the natural positions of targets. 

4. ### ğŸ˜†Denoising autoencoder (DAE)

   Denoising autoencoder (DAE) takes a partially corrupted input and aims to recover the original undistorted input.

   There are several ways to corrupt text:

   - ğŸ„â€â™‚ï¸Token masking: Randomly sampling tokens from theinput and replacing them with [MASK] elements.
   - ğŸ¤¾â€â™‚ï¸Token deletion: Randomly deleting tokens from the input. Different from token masking, the model needs to decide the positions of missing inputs.
   - ğŸ‹ï¸â€â™‚ï¸Text infilling: Like SpanBERT, a number of text spans are sampled and replaced with a single [MASK] token. Each span length is drawn from a Poisson distribution (Î» = 3). The model needs to predict how many tokens are missing from a
     span.
   - ğŸš´â€â™€ï¸Sentence permutation: Dividing a document into sentences based on full stops and shuffling these sentences in
     random order.
   - ğŸš£â€â™€ï¸Document rotation: Selecting a token uniformly at random and rotating the document so that it begins with that to-
     ken. The model needs to identify the real start

5. ### ğŸ™‚Contrastive learning (CTL)

   Contrastive learning assumes some observed pairs of text that are more semantically similar than randomly sampled text. The idea behind CTL is "learning by comparison".

   There are some recently proposed CTL tasks:

   - ğŸ„â€â™‚ï¸Deep InfoMax (DIM): Deep InfoMax is originally proposed for images, which improves the quality of the representation by maximizing the mutual information between an image representation and local regions of the image.
   - ğŸ¤¾â€â™‚ï¸Replaced token detection (RTD): Replaced token detection predicts whether a token is replaced given its surrounding context.
   - ğŸ‹ï¸â€â™‚ï¸Next sentence prediction (NSP): NSP trains the model to distinguish whether two input sentences are continuous segments from the training corpus.
   - ğŸš´â€â™€ï¸Sentence order prediction (SOP): SOP uses two consecutive segments from the same document as positive examples, and the same two consecutive segments but with their order swapped as negative examples.

6. ### ğŸ˜µOthers

   Apart from the above tasks, there are many other auxiliary pre-training tasks designated to incorporate factual knowledge, improve cross-lingual tasks, multi-modal applications, or other specific tasks.

   - ğŸ„â€â™‚ï¸Knowledge-enriched PTMs
   - ğŸ¤¾â€â™‚ï¸Multilingual and language-specific PTMs
   - ğŸ‹ï¸â€â™‚ï¸Multi-modal PTMs
   - ğŸš´â€â™€ï¸Domain-specific and task-specific PTMs



## 2ï¸âƒ£Pre-trained LMs

|                 æ¨¡å‹åç§°                 |        å‚æ•°é‡         |     è¯­è¨€     |         æ¨¡å‹æ¶æ„         |         è®­ç»ƒç›®æ ‡         |                           è®­ç»ƒè¯­æ–™                           | é“¾æ¥                                                         | å…¶ä»–                                               |
| :--------------------------------------: | :-------------------: | :----------: | :----------------------: | :----------------------: | :----------------------------------------------------------: | :----------------------------------------------------------- | -------------------------------------------------- |
|                BERT-base                 |         110M          | è‹±/ä¸­/å¤šè¯­è¨€ |   Transformer-encoder    |         MLM+NSP          |                BooksCorpus+Wikipedia  (16 GB)                | [github](https://github.com/google-research/bert)            |                                                    |
|                BERT-large                |         340M          |      è‹±      |   Transformer-encoder    |         MLM+NSP          |                    BooksCorpus+Wikipedia                     | [github](https://github.com/google-research/bert)            |                                                    |
|             StructBERT-base              |         110M          |      /       |   Transformer-encoder    |       MLM+NSP+SOP        |                    BooksCorpus+Wikipedia                     | [github](https://github.com/alibaba/AliceMind/tree/main/StructBERT) |                                                    |
|             StructBERT-large             |      340M, 330M       |    è‹±/ä¸­     |   Transformer-encoder    |       MLM+NSP+SOP        |                    BooksCorpus+Wikipedia                     | [github](https://github.com/alibaba/AliceMind/tree/main/StructBERT) |                                                    |
|                 SpanBERT                 |           /           |      /       |   Transformer-encoder    |         MLM(SBO)         |                    BooksCorpus+Wikipedia                     | [github](https://github.com/facebookresearch/SpanBERT)       |                                                    |
| ALBERT-base,large,  xlarge,<br />xxlarge |   12M,18M, 59M,233M   |    ä¸­/è‹±     |   Transformer-encoder    |         MLM+SOP          |                    BooksCorpus+Wikipedia                     | [github](https://github.com/google-research/albert)          |                                                    |
|         BART-base,<br />large,m          |  140M,  400M,  610M   |  è‹±/å¤šè¯­è¨€   |       Transformer        |           DAE            |                    BooksCorpus+Wikipedia                     | [github](https://github.com/pytorch/fairseq/tree/main/examples/bart) |                                                    |
|            RoBERTa-base,large            |      125M,  355M      |      è‹±      |   Transformer-encoder    |        MLM(åŠ¨æ€)         | BooksCorpus+Wikipedia   + CC-NEWS(76 GB)+ OPENWEBTEXT(38 GB)+STORIES(31 GB) | [github](https://github.com/pytorch/fairseq/tree/main/examples/roberta) |                                                    |
|                   XLM                    |           /           |    å¤šè¯­è¨€    |       Transformer        |         MLMï¼ŒTLM         | Wikipedia+MultiUN+IIT Bombay+OPUS websiteï¼š(EUbookshop, OpenSubtitles2018,  Tanzil, GlobalVoices) | [github](https://github.com/facebookresearch/XLM)            |                                                    |
|      ELECTRA-small,base<br />,large      |    14M,110M, 335M     |      è‹±      | Generator+ Discriminator |           RTD            | BooksCorpus+Wikipedia, BooksCorpus+Wikipedia+ ClueWeb+CommonCrawl+ Gigaword | [github](https://github.com/google-research/electra)         | [ä¸­æ–‡ç‰ˆ](https://github.com/ymcui/Chinese-ELECTRA) |
|                ERNIE-THU                 |           /           |      è‹±      | Transformer-encoder+  KG |    MLM+NSP+   KGèåˆ     |               BooksCorpus+Wikipedia  +Wikidata               | [github](https://github.com/thunlp/ERNIE)                    |                                                    |
|                ERNIE 3.0                 |          10B          |    ä¸­/è‹±     |  Transformer-encoder+KG  | MLM  (Knowledge Masking) |     Chinese text corpora (4TB) 11 different categories.      | [github](https://github.com/PaddlePaddle/ERNIE)              | æœªå…¬å¼€                                             |
|                   MASS                   |         120M          |     ç¿»è¯‘     |       Transformer        |       Seq2Seq-MLM        |                 WMT16+WMT News Crawl dataset                 | [github](https://github.com/microsoft/MASS)                  |                                                    |
|                Wu Dao 2.0                |  1.75T(æ¶µç›–å¾ˆå¤šæ¨¡å‹)  |  ä¸­/è‹±/åŒè¯­  |            \             |            \             |                      WuDaoCorpus 4.9 TB                      | [Official website](https://wudaoai.cn/home)                  | å¯ä¸‹è½½                                             |
|                CPM-2,MoE                 |       11B,198B        |    ä¸­/è‹±     |  Transformer-encoder+KG  |         Span MLM         |              WuDaoCorpus (zh:2.3 TB; en:300 GB)              | [Official website](https://wudaoai.cn/model/detail/CPM%E7%B3%BB%E5%88%97) |                                                    |
|                 UniLM v2                 |         110M          |      è‹±      |   Transformer-encoder    |         MLM+NSP          |    BooksCorpus+Wikipedia   + CC-NEWS+ OpenWebText+Stories    | [github](https://github.com/microsoft/unilm/tree/master/unilm) |                                                    |
|                    M6                    |         100B          | ä¸­æ–‡-å¤šæ¨¡æ€  |            \             |            \             |              images(1.9 TB),     texts(292 GB)               | [github]()                                                   |                                                    |
|    T5-small,<br />base, large, 3B,11B    | 60M,220M, 770M,3B,11B |      è‹±      |       Transformer        |         Span MLM         |                     Common Crawl(750 GB)                     | [github](https://github.com/google-research/text-to-text-transfer-transformer) |                                                    |
|                  CODEX                   |          12B          |     code     |   Transformer-decoder    |      åŸºäºGPT-3å¾®è°ƒ       |             Github Python files        (159 GB)              | [copilot](https://copilot.github.com/)                       |                                                    |
|             XLNet-base,large             |   similar with bert   |      è‹±      |   Transformer-encoder    |           PLM            |   BooksCorpus+Wikipedia +Giga5+ClueWeb 2012-B,Common Crawl   | [github](https://github.com/zihangdai/xlnet)                 | [ä¸­æ–‡ç‰ˆ](https://github.com/ymcui/Chinese-XLNet)   |
|                   GPT                    |         117M          |      è‹±      |   Transformer-decoder    |            LM            |                         BooksCorpus                          | [paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) |                                                    |
|                  GPT-2                   |         1.5B          |      è‹±      |   Transformer-decoder    |            LM            |                     Common Crawl(40 GB)                      | [github](https://github.com/openai/gpt-2)                    |                                                    |
|                  GPT-3                   |         175B          |      è‹±      |   Transformer-decoder    |            LM            | Common Crawl +  WebText datase+two internet-based books corpora+  English-language Wikipedia(570 GB-45 TB raw) | [Official website](https://beta.openai.com)                  | ä»˜è´¹                                               |



## **3ï¸âƒ£éƒ¨åˆ†å¤§æ¨¡å‹å¯ç”¨æ€§è°ƒç ”**

1. BERTï¼šå‚æ•°é‡110-340Mï¼ˆBertç³»åˆ—åŠå…¶å˜ç§ç­‰å°å‹PLMä¸€èˆ¬éƒ½å·²å¼€æºå‚æ•°ï¼Œå¯ä¸‹è½½æœ¬åœ°ä½¿ç”¨ï¼‰
2. T5ï¼šå‚æ•°é‡11Bï¼Œæ¨¡å‹å¤§å°çº¦15GBï¼Œå¯ä¸‹è½½æœ¬åœ°ä½¿ç”¨
3. GPT-2ï¼šå‚æ•°é‡1.5Bï¼Œä»˜è´¹
4. GPT-3ï¼šå‚æ•°é‡175Bï¼Œä»˜è´¹ï¼Œ0.7-0.01RMB/1K TOKENSï¼Œä»˜è´¹æ–¹å¼ï¼ˆä¸­å›½ä¸åœ¨ç”³è¯·åœ°åŒºï¼‰
5. åä¸ºç›˜å¤ï¼šå‚æ•°é‡200Bï¼Œæœªå¼€æ”¾ï¼Œåœ¨å’¨è¯¢
6. ç™¾åº¦ERNIE3.0ï¼šå‚æ•°é‡10Bï¼Œåœ¨å’¨è¯¢
7. RoBERTaï¼šå‚æ•°é‡125-355Mï¼Œå¯ä¸‹è½½ä½¿ç”¨
8. ALBERTï¼šå‚æ•°é‡125M
9. æ‚Ÿé“2.0-GLMï¼ˆGeneral Language Modelï¼‰ï¼šå‚æ•°10Bï¼Œç”³è¯·ä¸‹è½½ä½¿ç”¨
10. æ‚Ÿé“2.0-CPMï¼ˆChinese Pretrained Modelsï¼‰ï¼šå‚æ•°2.6,11,198Bï¼Œç”³è¯·ä¸‹è½½ä½¿ç”¨
11. BARTï¼šå‚æ•°é‡400Mï¼Œå¯ä¸‹è½½ä½¿ç”¨

![image-20220315121115658](https://raw.githubusercontent.com/BIT-Xu/pic/main/image-20220315121115658.png)

<div align = "center"><a href="https://arxiv.org/pdf/2107.13586.pdf?ref=https://githubhelp.com">Referenced from</a></div>



## 4ï¸âƒ£Transformeré¢„è®­ç»ƒæ¨¡å‹é€‚ç”¨ä»»åŠ¡æ±‡æ€»([Referenced from PaddleNLP](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers.html))

| Model                                                        | Sequence Classification | Token Classification | Question Answering | Text Generation | Multiple Choice |
| ------------------------------------------------------------ | ----------------------- | -------------------- | ------------------ | --------------- | --------------- |
| [ALBERT](https://arxiv.org/abs/1909.11942)                   | âœ…                       | âœ…                    | âœ…                  | âŒ               | âœ…               |
| [BART](https://arxiv.org/abs/1910.13461)                     | âœ…                       | âœ…                    | âœ…                  | âœ…               | âŒ               |
| [BERT](https://arxiv.org/abs/1810.04805)                     | âœ…                       | âœ…                    | âœ…                  | âŒ               | âœ…               |
| [BigBird](https://arxiv.org/abs/2007.14062)                  | âœ…                       | âœ…                    | âœ…                  | âŒ               | âœ…               |
| [Blenderbot](https://arxiv.org/pdf/2004.13637.pdf)           | âŒ                       | âŒ                    | âŒ                  | âœ…               | âŒ               |
| [Blenderbot-Small](https://arxiv.org/pdf/2004.13637.pdf)     | âŒ                       | âŒ                    | âŒ                  | âœ…               | âŒ               |
| [ConvBert](https://arxiv.org/abs/2008.02496)                 | âœ…                       | âœ…                    | âœ…                  | âœ…               | âœ…               |
| [CTRL](https://arxiv.org/abs/1909.05858)                     | âœ…                       | âŒ                    | âŒ                  | âŒ               | âŒ               |
| [DistilBert](https://arxiv.org/abs/1910.01108)               | âœ…                       | âœ…                    | âœ…                  | âŒ               | âŒ               |
| [ELECTRA](https://arxiv.org/abs/2003.10555)                  | âœ…                       | âœ…                    | âŒ                  | âŒ               | âœ…               |
| [ERNIE](https://arxiv.org/abs/1904.09223)                    | âœ…                       | âœ…                    | âœ…                  | âŒ               | âŒ               |
| [ERNIE-DOC](https://arxiv.org/abs/2012.15688)                | âœ…                       | âœ…                    | âœ…                  | âŒ               | âŒ               |
| [ERNIE-GEN](https://arxiv.org/abs/2001.11314)                | âŒ                       | âŒ                    | âŒ                  | âœ…               | âŒ               |
| [ERNIE-GRAM](https://arxiv.org/abs/2010.12148)               | âœ…                       | âœ…                    | âœ…                  | âŒ               | âŒ               |
| [GPT](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | âœ…                       | âœ…                    | âŒ                  | âœ…               | âŒ               |
| [LayoutLM](https://arxiv.org/abs/1912.13318)                 | âœ…                       | âœ…                    | âŒ                  | âŒ               | âŒ               |
| [LayoutLMV2](https://arxiv.org/abs/2012.14740)               | âŒ                       | âœ…                    | âŒ                  | âŒ               | âŒ               |
| [LayoutXLM](https://arxiv.org/abs/2104.08836)                | âŒ                       | âœ…                    | âŒ                  | âŒ               | âŒ               |
| [Mbart](https://arxiv.org/abs/2001.08210)                    | âœ…                       | âŒ                    | âœ…                  | âŒ               | âœ…               |
| [MobileBert](https://arxiv.org/abs/2004.02984)               | âœ…                       | âŒ                    | âœ…                  | âŒ               | âŒ               |
| [MPNet](https://arxiv.org/abs/2004.09297)                    | âœ…                       | âœ…                    | âœ…                  | âŒ               | âœ…               |
| [NeZha](https://arxiv.org/abs/1909.00204)                    | âœ…                       | âœ…                    | âœ…                  | âŒ               | âœ…               |
| [ReFormer](https://arxiv.org/abs/2001.04451)                 | âœ…                       | âŒ                    | âœ…                  | âŒ               | âŒ               |
| [RoBERTa](https://arxiv.org/abs/1907.11692)                  | âœ…                       | âœ…                    | âœ…                  | âŒ               | âŒ               |
| [RoFormer](https://arxiv.org/abs/2104.09864)                 | âœ…                       | âœ…                    | âœ…                  | âŒ               | âŒ               |
| [SKEP](https://arxiv.org/abs/2005.05635)                     | âœ…                       | âœ…                    | âŒ                  | âŒ               | âŒ               |
| [SqueezeBert](https://arxiv.org/abs/2006.11316)              | âœ…                       | âœ…                    | âœ…                  | âŒ               | âŒ               |
| [T5](https://arxiv.org/abs/1910.10683)                       | âŒ                       | âŒ                    | âŒ                  | âœ…               | âŒ               |
| [TinyBert](https://arxiv.org/abs/1909.10351)                 | âœ…                       | âŒ                    | âŒ                  | âŒ               | âŒ               |
| [UnifiedTransformer](https://arxiv.org/abs/2006.16779)       | âŒ                       | âŒ                    | âŒ                  | âœ…               | âŒ               |
| [XLNet](https://arxiv.org/abs/1906.08237)                    | âœ…                       | âœ…                    | âŒ                  | âŒ               | âŒ               |


